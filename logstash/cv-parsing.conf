# CV Parsing Pipeline
# Extracts skills, experience, education from CV text

input {
  http {
    port => 8080
    codec => json
    type => "cv"
  }
}

filter {
  # Extract skills using Grok patterns
  grok {
    match => {
      "cv_text" => [
        "(?i)(Python|Java|JavaScript|TypeScript|Go|Rust|C\+\+|C#|Ruby|PHP|Swift|Kotlin)",
        "(?i)(React|Angular|Vue|Django|Flask|FastAPI|Spring|Node\.js|Express)",
        "(?i)(PostgreSQL|MySQL|MongoDB|Redis|Elasticsearch|Cassandra)",
        "(?i)(Docker|Kubernetes|AWS|Azure|GCP|Terraform|Ansible)",
        "(?i)(Machine Learning|AI|Deep Learning|NLP|Computer Vision)",
        "(?i)(Git|CI/CD|Jenkins|GitLab|GitHub Actions)",
        "(?i)(Agile|Scrum|Kanban|TOGAF|Enterprise Architecture)"
      ]
    }
    tag_on_failure => []
  }

  # Extract years of experience
  grok {
    match => {
      "cv_text" => "(?i)(%{NUMBER:experience_years})\s*(years?|jahre)\s*(of\s*)?experience"
    }
    tag_on_failure => []
  }

  # Extract education level
  grok {
    match => {
      "cv_text" => [
        "(?i)(PhD|Doctorate|Doktor)",
        "(?i)(Master|M\.Sc\.|M\.A\.|MBA)",
        "(?i)(Bachelor|B\.Sc\.|B\.A\.)",
        "(?i)(Diplom|Diploma)"
      ]
    }
    tag_on_failure => []
  }

  # Normalize skills (lowercase, deduplicate)
  ruby {
    code => '
      skills = []
      event.to_hash.each do |key, value|
        if key.start_with?("skill_")
          skills << value.downcase
        end
      end
      event.set("skills_extracted", skills.uniq)
    '
  }

  # Add metadata
  mutate {
    add_field => {
      "[@metadata][index]" => "elastic_showcase_cv"
      "processed_at" => "%{+YYYY-MM-dd HH:mm:ss}"
      "pipeline" => "cv-parsing"
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:elasticsearch.railway.internal:9200}"]
    index => "%{[@metadata][index]}"
    document_id => "%{user_id}"
    action => "index"
  }

  # Debug output (optional)
  stdout {
    codec => rubydebug
  }
}
